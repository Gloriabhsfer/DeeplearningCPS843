{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "from datasetTrain import DatasetTrain\n",
    "from datasetVal import DatasetVal\n",
    "load_saver = False\n",
    "test_mode = False\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epoches = 100\n",
    "batch_size = 256\n",
    "num_display_steps = 15\n",
    "num_saver_epoches = 1\n",
    "save_dir = 'save_model/'\n",
    "log_dir = 'logs/'\n",
    "output_filename = 'final_output.txt'\n",
    "data_dir = 'data'\n",
    "test_dir = 'data/test'\n",
    "special_tokens  = {'<PAD>': 0, '<BOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
    "phases = {'train': 0, 'val': 1, 'test': 2}\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "n_inputs = 4096\n",
    "n_hidden = 256\n",
    "val_batch_size = 100\n",
    "n_frames = 80\n",
    "max_caption_len = 50\n",
    "forget_bias_red = 1.0\n",
    "forget_bias_gre = 1.0\n",
    "dropout_prob = 0.5\n",
    "class vedio2text:\n",
    "    def __init__(self, vocab_num = 0, lr = learning_rate):\n",
    "\n",
    "        self.vocab_num = vocab_num\n",
    "        self.learning_rate = lr\n",
    "        self.saver = None\n",
    "\n",
    "    def set_saver(self, saver):\n",
    "        self.saver = saver\n",
    "     \n",
    "    def build_model(self, feat, captions=None, cap_len=None, sampling=None, phase=0):\n",
    "\n",
    "        weights = {\n",
    "            'W_feat': tf.Variable( tf.random_uniform([n_inputs, n_hidden], -0.1, 0.1), name='W_feat'), \n",
    "            'W_dec': tf.Variable(tf.random_uniform([n_hidden, self.vocab_num], -0.1, 0.1), name='W_dec')\n",
    "        }\n",
    "        biases = {\n",
    "            'b_feat':  tf.Variable( tf.zeros([n_hidden]), name='b_feat'),\n",
    "            'b_dec': tf.Variable(tf.zeros([self.vocab_num]), name='b_dec')\n",
    "        }   \n",
    "        embeddings = {\n",
    "         'emb': tf.Variable(tf.random_uniform([self.vocab_num, n_hidden], -0.1, 0.1), name='emb')\n",
    "        }\n",
    "\n",
    "\n",
    "        batch_size = tf.shape(feat)[0]\n",
    "\n",
    "        if phase != phases['test']:\n",
    "            cap_mask = tf.sequence_mask(cap_len, max_caption_len, dtype=tf.float32)\n",
    "     \n",
    "        if phase == phases['train']: #  add noise\n",
    "            noise = tf.random_uniform(tf.shape(feat), -0.1, 0.1, dtype=tf.float32)\n",
    "            feat = feat + noise\n",
    "\n",
    "        if phase == phases['train']:\n",
    "            feat = tf.nn.dropout(feat, dropout_prob)\n",
    "\n",
    "        feat = tf.reshape(feat, [-1, n_inputs])\n",
    "        image_emb = tf.matmul(feat, weights['W_feat']) + biases['b_feat']\n",
    "        image_emb = tf.reshape(image_emb, [-1, n_frames, n_hidden])\n",
    "        image_emb = tf.transpose(image_emb, perm=[1, 0, 2])\n",
    "        \n",
    "        with tf.variable_scope('LSTM1'):\n",
    "            lstm_red = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=forget_bias_red, state_is_tuple=True)\n",
    "            if phase == phases['train']:\n",
    "                lstm_red = tf.contrib.rnn.DropoutWrapper(lstm_red, output_keep_prob=dropout_prob)    \n",
    "        with tf.variable_scope('LSTM2'):\n",
    "            lstm_gre = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=forget_bias_gre, state_is_tuple=True)\n",
    "            if phase == phases['train']:\n",
    "                lstm_gre = tf.contrib.rnn.DropoutWrapper(lstm_gre, output_keep_prob=dropout_prob)    \n",
    "\n",
    "        state_red = lstm_red.zero_state(batch_size, dtype=tf.float32)\n",
    "        state_gre = lstm_gre.zero_state(batch_size, dtype=tf.float32)\n",
    "\n",
    "\n",
    "        padding = tf.zeros([batch_size, n_hidden])\n",
    "\n",
    "        h_src = []\n",
    "        for i in range(0, n_frames):\n",
    "            with tf.variable_scope(\"LSTM1\"):\n",
    "                output_red, state_red = lstm_red(image_emb[i,:,:], state_red)\n",
    "            \n",
    "            with tf.variable_scope(\"LSTM2\"):\n",
    "                output_gre, state_gre = lstm_gre(tf.concat([padding, output_red], axis=1), state_gre)\n",
    "                h_src.append(output_gre) # even though padding is augmented, output_gre/state_gre's shape not change\n",
    "\n",
    "        h_src = tf.stack(h_src, axis = 0)\n",
    "\n",
    "        bos = tf.ones([batch_size, n_hidden])\n",
    "        padding_in = tf.zeros([batch_size, n_hidden])\n",
    "\n",
    "        logits = []\n",
    "        max_prob_index = None\n",
    "\n",
    "        cross_ent_list = []\n",
    "        for i in range(0, max_caption_len):\n",
    "\n",
    "            with tf.variable_scope(\"LSTM1\"):\n",
    "                output_red, state_red = lstm_red(padding_in, state_red)\n",
    "\n",
    "            if i == 0:\n",
    "                with tf.variable_scope(\"LSTM2\"):\n",
    "                    con = tf.concat([bos, output_red], axis=1)\n",
    "                    output_gre, state_gre = lstm_gre(con, state_gre)\n",
    "            else:\n",
    "                if phase == phases['train']:\n",
    "                    if sampling[i] == True:\n",
    "                        feed_in = captions[:, i - 1]\n",
    "                    else:\n",
    "                        feed_in = tf.argmax(logit_words, 1)\n",
    "                else:\n",
    "                    feed_in = tf.argmax(logit_words, 1)\n",
    "                with tf.device(\"/cpu:0\"):\n",
    "                    embed_result = tf.nn.embedding_lookup(embeddings['emb'], feed_in)\n",
    "                with tf.variable_scope(\"LSTM2\"):\n",
    "                    con = tf.concat([embed_result, output_red], axis=1)\n",
    "                    output_gre, state_gre = lstm_gre(con, state_gre)\n",
    "\n",
    "            logit_words = tf.matmul(output_gre, weights['W_dec']) + biases['b_dec']\n",
    "            logits.append(logit_words)\n",
    "\n",
    "            if phase != phases['test']:\n",
    "                labels = captions[:, i]\n",
    "                one_hot_labels = tf.one_hot(labels, self.vocab_num, on_value = 1, off_value = None, axis = 1) \n",
    "                cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logit_words, labels=one_hot_labels)\n",
    "                cross_entropy = cross_entropy * cap_mask[:, i]\n",
    "                cross_ent_list.append(cross_entropy)\n",
    "        \n",
    "        loss = 0.0\n",
    "        if phase != phases['test']:\n",
    "            cross_entropy_tensor = tf.stack(cross_ent_list, 1)\n",
    "            loss = tf.reduce_sum(cross_entropy_tensor, axis=1)\n",
    "            loss = tf.divide(loss, tf.cast(cap_len, tf.float32))\n",
    "            loss = tf.reduce_mean(loss, axis=0)\n",
    "\n",
    "        logits = tf.stack(logits, axis = 0)\n",
    "        logits = tf.reshape(logits, (max_caption_len, batch_size, self.vocab_num))\n",
    "        logits = tf.transpose(logits, [1, 0, 2])\n",
    "        \n",
    "        summary = None\n",
    "        if phase == phases['train']:\n",
    "            summary = tf.summary.scalar('training loss', loss)\n",
    "        elif phase == phases['val']:\n",
    "            summary = tf.summary.scalar('validation loss', loss)\n",
    "\n",
    "        return logits, loss, summary\n",
    "\n",
    "    def inference(self, logits):\n",
    "        \n",
    "        dec_pred = tf.argmax(logits, 2)\n",
    "        return dec_pred\n",
    "\n",
    "    def optimize(self, loss_op):\n",
    "\n",
    "        params = tf.trainable_variables()\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)#.minimize(loss_op)\n",
    "        gradients, variables = zip(*optimizer.compute_gradients(loss_op))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "        train_op = optimizer.apply_gradients(zip(gradients, params))\n",
    "        return train_op\n",
    "def train():\n",
    "    datasetTrain = DatasetTrain(data_dir, batch_size)\n",
    "    datasetTrain.build_train_data_obj_list()\n",
    "    vocab_num = datasetTrain.dump_tokenizer()\n",
    "    datasetVal = DatasetVal(data_dir, val_batch_size)\n",
    "    datasetVal.build_val_data_obj_list()\n",
    "    _ = datasetVal.load_tokenizer() # vocab_num are the same\n",
    "\n",
    "    train_graph = tf.Graph()\n",
    "    val_graph = tf.Graph()\n",
    "\n",
    "    with train_graph.as_default():\n",
    "        feat = tf.placeholder(tf.float32, [None, n_frames, n_inputs])\n",
    "        captions = tf.placeholder(tf.int32, [None, max_caption_len])\n",
    "        sampling = tf.placeholder(tf.bool, [max_caption_len])\n",
    "        cap_len = tf.placeholder(tf.int32, [None])\n",
    "        model = vedio2text(vocab_num=vocab_num, \n",
    "                    lr=learning_rate)\n",
    "        logits, loss_op, summary = model.build_model(feat, captions, cap_len, sampling, phases['train'])\n",
    "        dec_pred = model.inference(logits)\n",
    "        train_op = model.optimize(loss_op)\n",
    "\n",
    "        model.set_saver(tf.train.Saver(max_to_keep = 3))\n",
    "        init = tf.global_variables_initializer()\n",
    "    train_sess = tf.Session(graph=train_graph)\n",
    "\n",
    "    with val_graph.as_default():\n",
    "        feat_val = tf.placeholder(tf.float32, [None, n_frames, n_inputs])\n",
    "        captions_val = tf.placeholder(tf.int32, [None, max_caption_len])\n",
    "        cap_len_val = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "        model_val = vedio2text(vocab_num=vocab_num,lr=learning_rate)\n",
    "        logits_val, loss_op_val, summary_val = model_val.build_model(feat_val, \n",
    "                    captions_val, cap_len_val, phase=phases['val'])\n",
    "        dec_pred_val = model_val.inference(logits_val)\n",
    "\n",
    "        model_val.set_saver(tf.train.Saver(max_to_keep=3))\n",
    "    val_sess = tf.Session(graph=val_graph)\n",
    "\n",
    "    load = load_saver\n",
    "    if not load:\n",
    "        train_sess.run(init)\n",
    "    else:\n",
    "        saver_path = save_dir\n",
    "        latest_checkpoint = tf.train.latest_checkpoint(saver_path)\n",
    "        model.saver.restore(train_sess, latest_checkpoint)\n",
    "\n",
    "    ckpts_path = save_dir + \"save_net.ckpt\"\n",
    "    summary_writer = tf.summary.FileWriter(log_dir + '/train')\n",
    "    summary_writer.add_graph(train_graph)\n",
    "    summary_writer.add_graph(val_graph)\n",
    "\n",
    "    samp_prob = 0.6    \n",
    "    for epo in range(num_epoches):\n",
    "        datasetTrain.shuffle_perm()\n",
    "        num_steps = int( datasetTrain.batch_max_size / batch_size )\n",
    "        epo_loss = 0\n",
    "        for i in range(0, num_steps):\n",
    "            data_batch, label_batch, caption_lens_batch, id_batch = datasetTrain.next_batch()\n",
    "            samp = datasetTrain.schedule_sampling(samp_prob, caption_lens_batch)\n",
    "            if i % num_display_steps == 1:\n",
    "                # training \n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                _, loss, p, summ = train_sess.run([train_op, loss_op, dec_pred, summary], \n",
    "                                feed_dict={feat: data_batch,\n",
    "                                           captions: label_batch,\n",
    "                                           cap_len: caption_lens_batch,\n",
    "                                           sampling: samp},\n",
    "                                options=run_options)\n",
    "                summary_writer.add_summary(summ, global_step=(epo * num_steps) + i)\n",
    "                print(\"\\n Training Epoch \" + str(epo)  + \",......\")\n",
    "\n",
    "            else:\n",
    "                _, loss, p = train_sess.run([train_op, loss_op, dec_pred], \n",
    "                                feed_dict={feat: data_batch,\n",
    "                                           captions: label_batch,\n",
    "                                           cap_len: caption_lens_batch,\n",
    "                                           sampling: samp})\n",
    "            epo_loss += loss\n",
    "        av_loss = epo_loss/num_steps\n",
    "        print(\"\\n Finished Epoch \" + str(epo) + \\\n",
    "                \", (Training Loss (avarage loss in this epoch): \" + \"{:.4f}\".format(av_loss) + \")\")\n",
    "\n",
    "        if epo % num_saver_epoches == 0:\n",
    "            ckpt_path = model.saver.save(train_sess, ckpts_path, \n",
    "                global_step=(epo * num_steps) + num_steps - 1)\n",
    "            # validation\n",
    "            model_val.saver.restore(val_sess, ckpt_path)\n",
    "            print(\"\\n Validating Epoch \" + str(epo) +  \",......\")\n",
    "            \n",
    "            num_steps_val = int( datasetVal.batch_max_size / val_batch_size )\n",
    "            total_loss_val = 0 \n",
    "            for j in range(0, num_steps_val):\n",
    "\n",
    "                data_batch, label_batch, caption_lens_batch, id_batch = datasetVal.next_batch()\n",
    "                loss_val, p_val, summ = val_sess.run([loss_op_val, dec_pred_val, summary_val], \n",
    "                                        feed_dict={feat_val: data_batch,\n",
    "                                                   captions_val: label_batch,\n",
    "                                                   cap_len_val: caption_lens_batch})\n",
    "            \n",
    "                total_loss_val += loss_val\n",
    "                summary_writer.add_summary(summ, global_step=(epo * num_steps_val) + j)\n",
    "                \n",
    "            print(\"Validation: \" + str((j+1) * val_batch_size) + \"/\" + \\\n",
    "                    str(datasetVal.batch_max_size) + \", done...\" \\\n",
    "                    + \"Total Loss: \" + \"{:.4f}\".format(total_loss_val))\n",
    "    print('\\n\\nTraining finished!')\n",
    "def main(_):\n",
    "    train()\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run(main=main)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpsc843",
   "language": "python",
   "name": "cpsc843"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
